---
sidebar_position: 16
---

# Mistral Small 24B Instruct 2501

## ğŸš€ Model Overview

| Attribute           | Detail                             |
| :------------------ | :--------------------------------- |
| **Model Name**      | Mistral Small 24B Instruct 2501    |
| **Provider**        | Mistral AI                         |
| **Hosting**         | OTC                                |
| **Country**         | Germany ğŸ‡©ğŸ‡ª                        |
| **Parameter Count** | 24 billion                         |
| **Context Window**  | 32,768 tokens                      |

## âš™ï¸ Model Specifications

| Reasoning | Intelligence | Speed          | Input Formats         | Output Formats        | Price |
| :-------- | :----------- | :------------- | :-------------------- | :-------------------- | :---- |
| âŒ **No** | ğŸŸ¡ **Medium**| âš¡âš¡ **Medium** | âœ… Text               | âœ… Text               | â‚¬     |

## ğŸ“ Description

Mistral Small 24B Instruct 2501 is a 24-billion-parameter language model developed by Mistral AI. It is optimized for low-latency performance across common AI tasks and supports a context window of 32,768 tokens. It supports dozens of languages, including English, French, German, Spanish, Italian, Chinese, Japanese, Korean, Portuguese, Dutch, and Polish. Additionally, it offers native function calling and JSON output capabilities.
## âœ… Suitable Tasks

- Fast-response conversational agents
- Low-latency function calling
- Local inference for sensitive data handling

## âœ¨ Feature Support

| Feature                | Availability     |
| :--------------------- | :--------------- |
| **Streaming**          | âœ… Supported     |
| **Function Calling**   | âœ… Supported     |
| **Structured Outputs** | âœ… Supported     |
| **Fine-tuning**        | âŒ Not Supported |
| **Distillation**       | âŒ Not Supported |
| **Predicted Outputs**  | âœ… Supported     |
