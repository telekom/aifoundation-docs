---
sidebar_position: 17
---

# Mixtral 8x7B Instruct v0.1

## ğŸš€ Model Overview

| Attribute           | Detail                             |
| :------------------ | :--------------------------------- |
| **Model Name**      | Mixtral 8x7B Instruct v0.1         |
| **Provider**        | Mistral AI                         |
| **Hosting**         | OTC                                |
| **Country**         | European Union ğŸ‡ªğŸ‡º                 |
| **Parameter Count** | 12.9 billion active per token      |
| **Context Window**  | 32,768 tokens                      |

## âš™ï¸ Model Specifications

| Reasoning | Intelligence | Speed          | Input Formats         | Output Formats        | Price |
| :-------- | :----------- | :------------- | :-------------------- | :-------------------- | :---- |
| âŒ **No** | ğŸŸ¡ **Medium**| âš¡âš¡âš¡ **Fast**  | âœ… Text               | âœ… Text               | â‚¬     |

## ğŸ“ Description

Mixtral 8x7B Instruct v0.1 is a Sparse Mixture of Experts (SMoE) language model developed by Mistral AI. It consists of eight distinct expert groups, each with 7 billion parameters, totaling 46.7 billion parameters. However, during inference, only two experts are active per token, utilizing 12.9 billion parameters, which allows for efficient processing with the performance of a smaller model. The model supports a context window of 32,768 tokens and has been fine-tuned for instruction-following tasks, achieving a score of 8.3 on MT-Bench, comparable to GPT-3.5. It demonstrates strong performance in code generation and multilingual capabilities, handling languages such as English, French, Italian, German, and Spanish.

## âœ… Suitable Tasks

- Instruction Following
- Code Generation
- Multilingual Text Processing
- Creative Text Generation

## âœ¨ Feature Support

| Feature                | Availability     |
| :--------------------- | :--------------- |
| **Streaming**          | âœ… Supported     |
| **Function Calling**   | âœ… Supported     |
| **Structured Outputs** | âœ… Supported     |
| **Fine-tuning**        | âœ… Supported     |
| **Distillation**       | âŒ Not Supported |
| **Predicted Outputs**  | âœ… Supported     |
